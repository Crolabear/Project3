{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Import"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Things to keep in mind:\n",
      "#manually created the backup file to store the current copy of txt documents.\n",
      "##manually moved the name.txt file to the backup directory so that python can read it in.\n",
      "##Write a code to do these two things automatically\n",
      "\n",
      "\n",
      "#Import the necessary libraries: \n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import nltk\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from string import punctuation\n",
      "import re\n",
      "from collections import Counter\n",
      "import pickle\n",
      "from __future__ import division\n",
      "import math\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "import subprocess as subp\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The original data set consist of 2500 Excel files. Each excel file contains all the information inside one particular subreddit: it inclues Date posted, author, link to post, link to image, Likes/dislikes, title and actual post content. Four bash scripts were used to extract the necessary information used in this analysis.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This bash script creates empty text documents to be used when pickling output:\n",
      "print \"start script0\"\n",
      "subp.call(\"./scp0.sh\",shell=True)\n",
      "print 'end'\n",
      "\n",
      "# this script creates and stores all the subreddit names in a file called name.txt (created using scp0.sh)\n",
      "\n",
      "print \"start script1\"\n",
      "subp.call(\"./scp1.sh\",shell=True)\n",
      "print 'end'\n",
      "\n",
      "\n",
      "## scp1 also computes the ratio of text to image based subreddits.\n",
      "\n",
      "\n",
      "print \"start script2\"\n",
      "subp.call(\"./scp2.sh\",shell=True)\n",
      "print 'end'\n",
      "# scp2 creates the author document. stores all the author names\n",
      "\n",
      "print \"start script7\"\n",
      "subp.call(\"./scp7.sh\",shell=True)\n",
      "print 'end'\n",
      "\n",
      "## script 7 defines the zzauthor.txt doucment ; and store the info into zzauthor2.txt\n",
      "##\n",
      "## scp3 turns all csv into txt file.\n",
      "print \"start script3\"\n",
      "subp.call(\"./scp3.sh\",shell=True)\n",
      "print 'end'\n",
      "# scp3 strips away all the unnecessary variables in the original csv and store the ones i want to use in a text file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The scripts create three different text files: author, title/Name, content. Those text files will be loaded into python for further processing: remove common words, count up number of times it appears etc. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.download()\n",
      "# need to download the nltk package.\n",
      "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "#Load the stopwords: zzstop.txt is a textfile obtained \n",
      "#remove stop words\n",
      "zstop=open('zzstop.txt','rb').read().splitlines()\n",
      "zstop1=tokenizer.tokenize(zstop)\n",
      "\n",
      "#load the lemmatizer function\n",
      "lmtzr=WordNetLemmatizer()\n",
      "\n",
      "    \n",
      "## Function definitions:     \n",
      "# I have a list of names for all the subreddits.  This function opens up all the subreddit files and \n",
      "# reads in all the words. Stored them into a dictionary (only keeping the ones with a freq of 8 or more)\n",
      "def FreqTerms(filename):\n",
      "    filename=filename+'_inf.txt';\n",
      "    txt1=open(filename,'rb').read()\n",
      "    #txt1=open('zombies_inf.txt','rb').read()\n",
      "    txt3=re.sub('.,',\".\",txt1)\n",
      "    txt4=tokenizer.tokenize(txt3)\n",
      "    \n",
      "    ############\n",
      "    words=re.findall(r'\\w+',txt3)\n",
      "    low_words=[lmtzr.lemmatize(word) for word in words]\n",
      "    low_words=[word.lower() for word in words]\n",
      "    keep_words=[]\n",
      "    for word in low_words:\n",
      "        if word not in zstop:\n",
      "           keep_words.append(word) \n",
      "            \n",
      "\n",
      "    word_counts=Counter(keep_words)\n",
      "    DC=dict(word_counts)\n",
      "    DC={key:value for key, value in word_counts.items()\n",
      "        if value > 8\n",
      "        }\n",
      "    #use .keys() to get all the keys in the dictionary. The counts of each word is the value of the dictionary.\n",
      "    #DCTerms=DC.keys()\n",
      "    return(DC)\n",
      "\n",
      "#############\n",
      "\n",
      "\n",
      "\n",
      "#############\n",
      "#### This function is used to count the number of documents that contains a particular word.\n",
      "def findDoc(Term):\n",
      "    W=int()\n",
      "    for i in test:\n",
      "        # test is the file after i load in the pickle\n",
      "        if Term in i:\n",
      "            W=W+1\n",
      "    return(W)\n",
      "####\n",
      "#findDoc('zombie')  \n",
      "# will tell you how many documents has the word zombie in it.\n",
      "\n",
      "\n",
      "\n",
      "#############\n",
      "#############\n",
      "## Finding the normalized term freq for each word in every document. Returns a dictionary\n",
      "## \n",
      "def FindNor(y): \n",
      "    list0=y.values()\n",
      "    #list0\n",
      "    S=sum(list0)\n",
      "    Dic1=y.keys()\n",
      "    Dic2=map(lambda x: (x/S), list0)\n",
      "    Dic3=dict(zip(Dic1,Dic2))\n",
      "    return(Dic3)\n",
      "## D3 is the normalized freq\n",
      "###\n",
      "\n",
      "\n",
      "# the input TF is the term freq for a list of words\n",
      "# the keys are the elements common to both textfiles. the values are the Normalized freq for file 1, fil2, and \n",
      "# and the inverse document freq for each of the word  (See S3)\n",
      "# input should be dictionary\n",
      "# outputs a number, a dot product\n",
      "\n",
      "def FINDTFIDF(Diction):\n",
      "    SV=Diction.values()\n",
      "    IDF=map(lambda x: 1+math.log(2500/x[2]),SV)\n",
      "    TFIDF1=map(lambda x:x[0]*x[2] ,SV)\n",
      "    TFIDF2=map(lambda x:x[1]*x[2] ,SV)\n",
      "    NormTFIDF1=np.linalg.norm(TFIDF1,ord=2)\n",
      "    NormTFIDF2=np.linalg.norm(TFIDF2,ord=2)\n",
      "    #Prod=map(lambda x,y: sum(x*y),TFIDF1,TFIDF2)\n",
      "    Prod=[a*b for a,b in zip(TFIDF1, TFIDF2)]\n",
      "    Dprod = sum(Prod/(NormTFIDF1*NormTFIDF2))\n",
      "    return (Dprod)\n",
      "\n",
      "\n",
      "## This is the function i use to compute the similarity matrix. \n",
      "## it takes in 2 dictionaries (representing 2 documents)\n",
      "def FindSimi(dic1,dic2,test):\n",
      "#The inputs dictionary 1 and 2 are dictionaries of words and their frequencies for doc. 1 and doc.2\n",
      "# also bring in the normalized freq: D3, F3:    \n",
      "    D3=FindNor(dic1)\n",
      "    F3=FindNor(dic2)\n",
      "# Find the shared words:\n",
      "# SK represents shared keys. AKA words shared by both documents\n",
      "    SK=D3.viewkeys()&F3.viewkeys()\n",
      "#This is the number of documents u find this word in\n",
      "    DicTF={key:(findDoc(key)) for key in SK}\n",
      "##note:\n",
      "##When I display DicTF, the output is a sorted dictionary. but it does correspond to the right freq count\n",
      "\n",
      "\n",
      "#S3 is a dictionary with key=shared words, value1=frequency, value2= doc. frequency.\n",
      "#doc. frequency is how many documents have this word in it.\n",
      "# freq is the number of times this word appears in this particular document.\n",
      "    S3={key : (D3[key],F3[key],DicTF[key]) for key in SK}\n",
      "#SV=S3.values()\n",
      "#S3 is the inverse doc freq for 2 docs.\n",
      "# let's bring in the original normalized freq.\n",
      "    a=FINDTFIDF(S3)\n",
      "    return(a)\n",
      "# lastly, call the FINDTFIDF function to compute a score (by taking a dot product)\n",
      "# EXAMPLE\n",
      "#z=SimiTitle('alcohol',Title)\n",
      "## z is a vector of length 2500. The elements where z is no 0 means there is a match for z, that is\n",
      "## that particular title is somewhat similar to z in a sense\n",
      "\n",
      "##### End of functions for text cleaning\n",
      "\n",
      "###########################\n",
      "###########################\n",
      "\n",
      "## The TFIDF approach function\n",
      "#### Functions used to find tfidf using map\n",
      "def OneDocIDF(oneDoc,fullDoc):\n",
      "    aa= map(lambda x:FindSimi(x,oneDoc,fullDoc),fullDoc)\n",
      "    return(aa)\n",
      "\n",
      "def ManyDocIDF(collection,fullDoc):\n",
      "    aa=map(lambda x: OneDocIDF(x,fullDoc),collection)\n",
      "    return(aa)\n",
      "####\n",
      "####\n",
      "\n",
      "\n",
      "#####  Simple word approach  \n",
      "## Functions for getting simple word counts\n",
      "####\n",
      "\n",
      "##### This function Finds the shared keywords for any two given dictionary (documents)\n",
      "def FindSharedKey(dic1,dic2):\n",
      "    ABkey=dic1.viewkeys()&dic2.viewkeys()\n",
      "    leng=min(len(dic1),len(dic2))\n",
      "    prop=len(ABkey)/(leng+1)\n",
      "    return(prop)\n",
      "## used to find the simple word count, see how many terms two documents share\n",
      "\n",
      "\n",
      "#Apply the FindsharedKey function and do it for the 2500*2500 pairs\n",
      "\n",
      "def PropWordShared(dic1,test):\n",
      "    propShared=map(lambda x: FindSharedKey(dic1,x),test)\n",
      "    return(propShared)\n",
      "\n",
      "def SimiDoc(test):\n",
      "    SharedWordProp=map(lambda x: PropWordShared(x,test),test)\n",
      "    return(SharedWordProp)\n",
      "#######\n",
      "######\n",
      "######  End of functions for analyzing Contents\n",
      "\n",
      "\n",
      "## Functions for getting title similarities:\n",
      "####\n",
      "#######\n",
      "\n",
      "\n",
      "## removes all the numeric numbers in the title\n",
      "def remNum(x):\n",
      "    y=re.sub('[0-9]',\"\",x)\n",
      "    return(y)\n",
      "#Title=map(remNum,Names)\n",
      "\n",
      "\n",
      "# cut the title up into strings. output a set of strings, different parts of the title. \n",
      "# example: alcohol -> al, alc, alco, alcoh, alcoho, alcohol\n",
      "def sliceup(x):\n",
      "    leng=len(x)-2\n",
      "    y=range(leng)\n",
      "    z=map(lambda a: x[0:a+3], y)\n",
      "    # can make the match more harsh by increasing th size of the sub segment.\n",
      "    return(z)\n",
      "\n",
      "\n",
      "# Use the strings to compare two words\n",
      "def FindCommon(word,slices):\n",
      "    L=len(slices)\n",
      "    f=map(lambda x:(int(bool(re.search(x,word)))/L),slices)\n",
      "    return(sum(f))\n",
      "\n",
      "#Assign a score to the two titles base on how similar they are\n",
      "def SimiTitle(word, Title):\n",
      "    # compares a particular word to the each elements in the title.\n",
      "    g=sliceup(word)\n",
      "    h=map(lambda x:FindCommon(x,g) ,Title)\n",
      "    return(h)\n",
      "\n",
      "###End of function definition."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Here we will run the functions we defined earlier and create different dictionaries."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#read in the file names (it's a file that sits in the local directory, a list of names for all the documents.)\n",
      "Names=open('name.txt','rb').read().splitlines()\n",
      "\n",
      "\n",
      "########----  These are codes to run to generate the necessary textdoc; and pickle them in a text file.\n",
      "## Run to make a dictionary: zzDic.txt\n",
      "########\n",
      "\n",
      "CombinedDic=[]\n",
      "for f in Names:\n",
      "    CombinedDic.append(FreqTerms(f))\n",
      "    print f\n",
      "len(CombinedList)    \n",
      "zzDic=open('zzDic.txt','wb')\n",
      "pickle.dump(CombinedDic, zzDic)\n",
      "zzDic.close()\n",
      "\n",
      "## Pickled\n",
      "\n",
      "\n",
      "## will take a lot of memory to run\n",
      "Title=map(remNum,Names)\n",
      "SimScore1=[]\n",
      "SimScore1=map(lambda x: SimiTitle(x,Title),Title)\n",
      "\n",
      "## created a zzsimscore.txt file in the directory.\n",
      "zzSim1=open('zzsimscore.txt','wb')\n",
      "pickle.dump(SimScore1, zzSim1)\n",
      "zzSim1.close()\n",
      "#cool pickled. I can just read it in next time i need it\n",
      "\n",
      "# also take a while to run\n",
      "gg=SimiDoc(test)\n",
      "## created a zzsimplecount.txt file in the directory.\n",
      "zzSim2=open('zzsimplecount.txt','wb')\n",
      "pickle.dump(gg, zzSim2)\n",
      "zzSim2.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Load in the pickled dictionary\n",
      "\n",
      "## Contents\n",
      "test4=open('zzDic.txt','rb')\n",
      "test=pickle.load(test4)\n",
      "\n",
      "#load in the title info so i can use\n",
      "#simscore is info title\n",
      "SimTitle=open('zzsimscore1.txt','rb')\n",
      "Simscore1=pickle.load(SimTitle)\n",
      "\n",
      "## simplecount is for simple word freq.\n",
      "SimCount=open('zzsimplecount.txt','rb')\n",
      "gg=pickle.load(SimCount)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Making graphs: See the next notebook\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}